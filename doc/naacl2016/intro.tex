
\section{Introduction}
\label{introduction}
The goal of automatic knowledge base construction (AKBC) is building a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented~\citep{NELL,yago,freebase}. AKBC supports downstream reasoning at a high level about extracted entities and their relations, and thus has broad-reaching applications to a variety of domains.

One challenge in AKBC is aligning knowledge from a structured KB with a text corpus in order to perform supervised learning through \emph{distant supervision}. \emph{Universal schema}~\citep{limin} along with its extensions~\citep{yao2013universal,vector_pra,neelakantan2015compositional,logicmfnaacl15,toutanova2015representing} avoids this issue of alignment by jointly embedding KB relations, entities and text patterns. This allows information to propagate between KB annotation and corresponding textual evidence without explicit sentence-relation alignment.

Previous approaches to universal schema express each text relation as a distinct item to be embedded. This harms its ability to generalize, making it impossible to process inputs not precisely seen at training time. However, for large-scale applications we are interested in generalizing to new text patterns, new entities, and even new domains. We focus on the extreme example of domain adaptation to a completely new language, which may have limited resources or labeled data such as treebanks, and only rarely a KB with adequate coverage. 

This paper leverages universal schema  to train a deep sentence encoder that captures the compositional semantics of textual relations, allowing for prediction on inputs never seen before. We demonstrate the generality of our method by evaluating in a multilingual transfer learning setting, extracting relations from a corpus in a new language with no coverage in an existing KB, requiring only that the entities in the text corpora for two languages overlap, as depicted in Figure \ref{tab:multilingual-corpora}.

We further improve our models by tying a small set of word embeddings across languages using only simple knowledge about word-level translations, learning to embed semantically similar textual patterns from different languages into the same latent space.

In extensive experiments on the TAC Knowledge Base Population (KBP) slot-filling benchmark we perform relation extraction in Spanish with no labeled data or direct Spanish-KB overlap, demonstrating that our approach is well-suited for broad-coverage AKBC in low-resource languages and domains. Interestingly, we also find that joint training with Spanish improves English accuracy. 

\begin{figure}[h!]
\begin{center}
\vspace{-1.9069cm}

\def\firstcircle{(0,0) circle (1.5cm)}
\def\secondcircle{(0:2cm) circle (1.5cm)}
\def\midline{[line width=1pt, dashed] node[label={[label distance=-3cm]-15:in KB}] {} node[label={[label distance=-3.5cm]15:not in KB}] {} (-3,0) -- (5,0)}
\begin{tikzpicture}

	\begin{scope}
	  \clip \firstcircle (-3,3) rectangle (3,3);
	  \fill[pattern=north west lines, pattern color=black!70] \firstcircle;
	\end{scope}
	\begin{scope}
	  \clip \secondcircle (0,0) rectangle (4,4);
	  \fill[pattern=north east lines, pattern color=black!70] \secondcircle;
	\end{scope}
	\begin{scope}
	  \clip \firstcircle;
	  \clip (0.5,0) rectangle (3,2);
	  \fill[white] \secondcircle;
	\end{scope}
	\draw \firstcircle node[above=1.5cm] {English};
	\draw \secondcircle node[above=1.5cm] {Low-resource};
	\draw \midline;

\end{tikzpicture}
\caption{Splitting the entities in a multilingual AKBC training set into parts. We only require that entities in the two corpora overlap. Remarkably, we can train a model for the low-resource language even if entities in the low-resource language do not occur in the KB. \label{tab:multilingual-corpora}}
\end{center}
\vspace{-.4cm}
\end{figure}

\section{Background \label{sec:background}}

AKBC extracts unary attributes of the form (\textit{subject}, \textit{attribute}), typed binary relations of the form (\textit{subject}, \textit{relation}, \textit{object}), or higher-order relations. We refer to subjects and objects as \textit{entities}. This work focuses solely on extracting binary relations, though many of our techniques generalize naturally to unary prediction. Generally, for example in Freebase~\citep{freebase}, higher-order relations are expressed in terms of collections of binary relations.

We now describe prior work on approaches to AKBC. They all aim to predict (\emph{s, r, o}) triples, but differ in terms of: (1) input data leveraged, (2) types of annotation required, (3) definition of relation label schema, and (4) whether they are capable of predicting relations for entities unseen in the training data. Note that all of these methods require pre-processing to detect entities, which may result in additional KB construction errors.

\subsection{Relation Extraction as Link Prediction \label{sec:prediction}}
A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges~\citep{yago,freebase}. In the case of \emph{knowledge graph completion}, the task is akin to link prediction, assuming an initial set of (\emph{s, r, o}) triples. See~\citet{nickel2015review} for a review. No accompanying text data is necessary, since links can be predicted using properties of the graph, such as transitivity. In order to generalize well, prediction is often posed as low-rank matrix or tensor factorization. A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form~\citep{rescal,DBLP:journals/corr/Garcia-DuranBUG15,bishan,transe,wang2014knowledge,lin2015learning}, or non-linear interactions between $s$, $r$, and $o$~\citep{socherkb}.
Other approaches model the compositionality of multi-hop paths, typically for question answering \citep{bordes2014question,gu2015traversing,neelakantan2015compositional}.

\subsection{Relation Extraction as Sentence Classification}
\label{seq:dist}

Here, the training data consist of (1) a text corpus, and (2) a KB of seed facts with provenance, ie. supporting evidence, in the corpus. Given individual an individual sentence, and pre-specified entities, a classifier predicts whether the sentence expresses a relation from a target schema. To train such a classifier, KB facts need to be aligned with supporting evidence in the text, but this is often challenging. For example, not all sentences containing Barack and Michelle Obama state that they are married. A variety of one-shot and iterative methods have addressed the alignment problem~\citep{bunescu2007learning,distant_supervision,riedel2010modeling,yao2010collective,hoffmann2011knowledge,surdeanu2012multi,min2013distant,zengdistant}.
An additional degree of freedom in these approaches is whether they classify individual sentences or predicting at the corpus level by aggregating information from all sentences containing a given pair of entities before prediction. The former approach is often preferrable in practice, due to the simplicity of independently classifying individual sentences and the ease of associating each prediction with a provenance.

\subsection{Open-Domain Relation Extraction}
\label{sec:openIE}
In the previous two approaches, prediction is carried out with respect to a fixed schema $R$ of possible relations $r$. This may overlook salient relations that are expressed in the text but do not occur in the schema. In response, \textit{open-domain} information extraction (OpenIE) lets the text speak for itself: $R$ contains all possible patterns of text occuring between entities $s$ and $o$~\citep{openie,etzioni2008open,resolver}. These are obtained by filtering and normalizing the raw text. The approach offers impressive coverage, avoids issues of distant supervision, and provides a useful exploratory tool. On the other hand, OpenIE predictions are difficult to use in downstream tasks that expect information from a fixed schema. 

Table~\ref{tab:patterns} provides examples of OpenIE patterns. The examples in row two and three illustrate relational contexts for which similarity is difficult to be captured by an OpenIE approach because of their syntactically complex constructions. This motivates the technique in Section~\ref{sec:encoder}, which uses a deep architecture applied to the raw tokens, instead of rigid rules for normalizing text to obtain patterns.

\begin{table}[h!]
\small
\begin{center}
%\hspace{-1cm}
\begin{tabular}{|p{4.85cm}| p{2.37cm} | }
\hline
%Relation &
Sentence (context tokens italicized) & OpenIE pattern\\ \hline
%per:siblings &
{\bf Khan} \emph{'s younger sister,} {\bf Annapurna Devi}, who later married Shankar, developed into an equally accomplished master of the surbahar, but custom prevented her from performing in public. &  \argOne 's * sister \argTwo \\ \hline

%per:cities\_of\_residence &
A professor emeritus at Yale, {\bf Mandelbrot} \emph{was born in Poland but as a child moved with his family to} {\bf Paris} where he was educated. &  \argOne * moved with * family to \argTwo \\ \hline

%per:cities\_of\_residence &
{\bf Kissel} \emph{was born in Provo, Utah, but her family also lived in} {\bf Reno}. & \argOne * lived in \argTwo \\  \hline
\end{tabular}
\caption{Examples of sentences expressing relations. Context tokens (italicized) consist of the text occurring between entities (bold) in a sentence. OpenIE patterns are obtained by normalizing the context tokens using hand-coded rules. The top example expresses the per:siblings relation and the bottom two examples both express the per:cities\_of\_residence  relation. \label{tab:patterns}}
\end{center}
\vspace{-.4cm}
\end{table}

\subsection{Universal Schema}
When applying Universal Schema~\citep{limin} (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives.  By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section~\ref{seq:dist}. 

~\citet{limin} augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges may be able to exploit regularities of the OpenIE edges to improve modeling of the labels from the target schema. 

The data still consist of $(s,r,o)$ triples, which can be predicted using link-prediction techniques such as low-rank factorization.~\citet{limin} explore a variety of approximations to the 3-mode $(s,r,o)$ tensor. One such probabilistic model is:
\begin{equation}
\Prob \left((s,r,o)\right) = \sigma\left( u_{s,o}^\top v_r \right), \label{eq:US-prob}
\end{equation}

where  $\sigma()$ is a sigmoid function, $u_{s,o}$ is an embedding of the entity pair $(s,o)$, and $v_r$ is an embedding of the relation $r$, which may be an OpenIE pattern or a relation from the target schema. All of the exposition and results in this paper use this factorization, though many of the modeling techniques we present later could be applied easily to the other factorizations described in ~\citet{limin}. Note that learning unique embeddings for each OpenIE relations does not guarantee that similar patterns, such as the final two in Table~\ref{tab:patterns}, will be embedded similarly.

As with most of the techniques in Section~\ref{sec:prediction}, the data only consist of positive examples of edges. The absence of an annotated edge does not imply that the edge is false. In fact, we seek to predict some of these missing edges as true. ~\citet{limin} employ the Bayesian Personalized Ranking (BPR) approach of~\citet{rendle2009bpr}, which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of observed triples above unobserved triples.


Recently, \citet{toutanova2015representing} extended USchema to not learn individual pattern embeddings $v_r$, but instead to embed text patterns using a deep architecture applied to word tokens. This shares statistical strength between OpenIE patterns with similar words. We leverage this approach in Section~\ref{sec:encoder}. Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns~\citep{pra,pra_second,vector_pra,neelakantan2015compositional}.



