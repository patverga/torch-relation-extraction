\documentclass{article} % For LaTeX2e
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{tikz}
% \usepackage{natbib}
\usetikzlibrary{shapes,backgrounds,patterns}


\newcommand{\Prob}{\mathbb{P}}
\newcommand{\todo}[1]{{\bf [[}\textcolor{blue}{ todo: #1}{\bf ]]}}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% named as such because `\arg1' apparently isn't valid
\newcommand{\argOne}{\emph{arg1}\xspace}
\newcommand{\argTwo}{\emph{arg2}\xspace}

\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\citet}[1]{\newcite{#1}}



% \title{Broad Coverage Knowledge Base Construction using Cross-Lingual Transfer}
%\title{No KB? No Problem! Transfer Learning for Multilingual Relation Extraction}
\title{Multilingual Relation Extraction using \\Compositional Universal Schema}
%\title{Transfer Learning for Multilingual \\Knowledge Base Construction}
% \title{Broad Coverage Knowledge Base Construction using Multilingual Embeddings}

\author{Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth \& Andrew McCallum \\
College of Information and Computer Sciences\\
University of Massachusetts, Amherst\\
% Amherst, MA 01002, USA \\
\texttt{\{pat, belanger, strubell, beroth, mccallum\}@cs.umass.edu} \\
}


%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
When building a knowledge base (KB) of entities and relations from multiple structured KBs and text, \emph{universal schema} represents the union of all input schema, by jointly embedding all relation types from input KBs as well as textual patterns expressing relations.  In previous work, textual patterns are parametrized as a single embedding, preventing generalization to unseen textual patterns. In this paper we employ an LSTM to compositionally capture the semantics of relational text. In extensive experiments on the English and Spanish TAC KBP benchmark, our techniques provide substantial accuracy improvements and we achieve state-of-the-art performance on TAC 2013 SlotFilling using no handwritten patterns. We demonstrate the flexibility of our approach by evaluating in a multilingual setting, in which the English training data entities overlap with the seed KB, but the Spanish text does not. Additional improvements are obtained by tying word embeddings across languages. Furthermore we find that training with the additional non-overlapping Spanish also improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in low-resource languages and domains.
\end{abstract}


% intro + background
\input{intro}

% uschema rel extraction stuff
\input{uschema}

% multilingual joint embedding
\input {multilingual}

% experiemnts
\input {experiments}

% results
\input {results}
\section{Conclusion}

By jointly embedding English and Spanish KBs, we can train an accurate Spanish relation extraction model using no direct annotation for relations in the Spanish data. This approach has the added benefit of providing significant accuracy improvements for the English model, obtaining nearly state-of-the-art accuracy on the 2013 TAC KBC slot filling task, while using substantially fewer hand-coded rules than alternative systems. By using deep sentence encoders, we can perform prediction for arbitrary input text and for entities unseen in training. Sentence encoders also provides opportunities to improve cross-lingual transfer learning by sharing word embeddings across languages. In future work we will apply this model to many more languages and domains besides newswire text. We would also like to avoid the entity detection problem by using a deep architecture to both identify entity mentions and identify relations between them.

\subsubsection*{Acknowledgments}
Many thanks to Arvind Neelakantan and Noah Smith for good ideas and discussions. We also appreciate a generous hardware grant from nVidia. This work was supported in part by the Center for Intelligent Information Retrieval, in part by Defense Advanced Research Projects Agency (DARPA) under agreement \#FA8750-13-2-0020 and contract \#HR0011-15-2-0036, and in part by the National Science Foundation (NSF) grant numbers DMR-1534431, IIS-1514053 and CNS-0958392. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon, in part by DARPA via agreement \#DFA8750-13-2-0020 and NSF grant \#CNS-0958392. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

\bibliography{sources}
\bibliographystyle{naaclhlt2016}

\appendix
\input{appendix}




\end{document}
